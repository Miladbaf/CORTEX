{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611e9c11",
   "metadata": {},
   "source": [
    "# CORTEX â€” Stage I: Beam Angle Prediction\n",
    "\n",
    "> **Conv Front-End Â· RoPE Â· Causal Transformer Â· Circular Regression**\n",
    "\n",
    "This notebook trains **CORTEX Stage I**, which predicts optimal TX/RX beam angles for THz UAV communications from mobility time-series features.  \n",
    "It outputs two normalized 3-D unit vectors (one per antenna) and optimises a **Wrapped Cauchy loss** that handles circular angle geometry correctly.\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Input [B, T, F]\n",
    "  â†’ Conv Front-End  (2Ã— strided Conv1d + GELU + BN)\n",
    "  â†’ Causal Transformer  (NÃ— RoPE self-attention + FFN)\n",
    "  â†’ MLP head  â†’ 6-D output â†’ L2-normalise â†’ TX vec (3D) | RX vec (3D)\n",
    "```\n",
    "\n",
    "---\n",
    "**Outputs saved to Google Drive** (optional) or local `outputs/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b9868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / upgrade required packages\n",
    "!pip install torch pandas numpy matplotlib tqdm scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3562b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math, json, warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df8cde",
   "metadata": {},
   "source": [
    "## 1 Â· Setup: Output Directories\n",
    "\n",
    "Run **either** the Google Colab block (mounts Drive) **or** the local block below.  \n",
    "Skip whichever doesn't apply to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Option A: Google Colab + Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Comment out this block if running locally.\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    OUTPUT_ROOT = \"/content/drive/MyDrive/CORTEX_Stage1\"\n",
    "    print(\"âœ… Google Drive mounted\")\n",
    "except ImportError:\n",
    "    # â”€â”€ Option B: Local â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    OUTPUT_ROOT = \"outputs\"\n",
    "    print(\"â„¹ï¸  Running locally â€” outputs go to ./outputs/\")\n",
    "\n",
    "MODELS_DIR  = os.path.join(OUTPUT_ROOT, \"models\")\n",
    "RESULTS_DIR = os.path.join(OUTPUT_ROOT, \"results\")\n",
    "PLOTS_DIR   = os.path.join(OUTPUT_ROOT, \"plots\")\n",
    "\n",
    "for d in [MODELS_DIR, RESULTS_DIR, PLOTS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"Models  â†’ {MODELS_DIR}\")\n",
    "print(f\"Results â†’ {RESULTS_DIR}\")\n",
    "print(f\"Plots   â†’ {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdee247",
   "metadata": {},
   "source": [
    "## 2 Â· Load Data\n",
    "\n",
    "Point `CSV_PATH` to your mobility CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4784312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Set your CSV path here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH = \"data/mobility.csv\"          # <-- change this\n",
    "\n",
    "# Auto-detect if not found at the explicit path\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    # Look in Google Drive common locations\n",
    "    for candidate in glob.glob(\"/content/drive/**/*.csv\", recursive=True):\n",
    "        CSV_PATH = candidate\n",
    "        print(f\"Auto-detected: {CSV_PATH}\")\n",
    "        break\n",
    "    else:\n",
    "        # Colab file upload fallback\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            print(\"No CSV found â€” please upload your file:\")\n",
    "            uploaded = files.upload()\n",
    "            CSV_PATH = list(uploaded.keys())[0]\n",
    "        except ImportError:\n",
    "            raise FileNotFoundError(\n",
    "                f\"CSV not found at '{CSV_PATH}'. \"\n",
    "                \"Set CSV_PATH to the correct path.\"\n",
    "            )\n",
    "\n",
    "data = pd.read_csv(CSV_PATH)\n",
    "print(f\"\\nLoaded {len(data):,} rows | {data['scenario'].nunique():,} scenarios\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "\n",
    "# Per-motion summary\n",
    "print(f\"\\n{'Motion Type':<20} {'Samples':>10} {'Scenarios':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for mt in sorted(data[\"motion_type\"].unique()):\n",
    "    md = data[data[\"motion_type\"] == mt]\n",
    "    print(f\"{mt:<20} {len(md):>10,} {md['scenario'].nunique():>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813314f1",
   "metadata": {},
   "source": [
    "## 3 Â· Circular Angle Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc19b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angles_to_unit_vectors(phi, theta):\n",
    "    \"\"\"(phi, theta) in degrees â†’ 3-D unit vectors.\"\"\"\n",
    "    phi_r, theta_r = np.deg2rad(phi), np.deg2rad(theta)\n",
    "    return np.stack([\n",
    "        np.sin(theta_r) * np.cos(phi_r),\n",
    "        np.sin(theta_r) * np.sin(phi_r),\n",
    "        np.cos(theta_r),\n",
    "    ], axis=-1)\n",
    "\n",
    "\n",
    "def unit_vectors_to_angles(vecs):\n",
    "    \"\"\"3-D unit vectors â†’ (phi, theta) in degrees.\"\"\"\n",
    "    x, y, z = vecs[..., 0], vecs[..., 1], vecs[..., 2]\n",
    "    theta = np.rad2deg(np.arccos(np.clip(z, -1, 1)))\n",
    "    phi   = np.rad2deg(np.arctan2(y, x))\n",
    "    phi   = np.where(phi < 0, phi + 360, phi)\n",
    "    return phi, theta\n",
    "\n",
    "\n",
    "def circular_mae(pred, true):\n",
    "    \"\"\"Circular mean absolute error (degrees, wraps at 360Â°).\"\"\"\n",
    "    diff = np.abs(pred - true)\n",
    "    diff = np.where(diff > 180, 360 - diff, diff)\n",
    "    return float(np.mean(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cafaf",
   "metadata": {},
   "source": [
    "## 4 Â· Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c253bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = [\n",
    "    \"time\", \"scenario\", \"motion_type_encoded\", \"distance\",\n",
    "    \"velocity\", \"acceleration\", \"los_angle_tx\", \"los_angle_rx\",\n",
    "    \"los_ele_tx\", \"los_ele_rx\", \"velocity_angle\", \"mobility_factor\",\n",
    "]\n",
    "\n",
    "\n",
    "class MobilityTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=10,\n",
    "                 scaler=None, label_encoder=None, fit=True):\n",
    "        self.sequence_length = sequence_length\n",
    "        data = data.copy()\n",
    "\n",
    "        if label_encoder is None:\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            data[\"motion_type_encoded\"] = self.label_encoder.fit_transform(data[\"motion_type\"])\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "            data[\"motion_type_encoded\"] = self.label_encoder.transform(data[\"motion_type\"])\n",
    "\n",
    "        # Targets â†’ 6-D unit vectors\n",
    "        vec_tx = angles_to_unit_vectors(\n",
    "            data[\"phi_tx_optimal\"].values, data[\"theta_tx_optimal\"].values)\n",
    "        vec_rx = angles_to_unit_vectors(\n",
    "            data[\"phi_rx_optimal\"].values, data[\"theta_rx_optimal\"].values)\n",
    "        target_vectors = np.concatenate([vec_tx, vec_rx], axis=-1)\n",
    "\n",
    "        self.original_angles = np.stack([\n",
    "            data[\"phi_tx_optimal\"].values, data[\"theta_tx_optimal\"].values,\n",
    "            data[\"phi_rx_optimal\"].values, data[\"theta_rx_optimal\"].values,\n",
    "        ], axis=-1)\n",
    "\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            features_scaled = self.scaler.fit_transform(data[FEATURE_COLS])\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            features_scaled = self.scaler.transform(data[FEATURE_COLS])\n",
    "\n",
    "        # Sliding windows per scenario\n",
    "        self.sequences, self.targets, self.target_angles = [], [], []\n",
    "        data_idx = data.reset_index(drop=True)\n",
    "        feat_df  = pd.DataFrame(features_scaled, index=data_idx.index)\n",
    "\n",
    "        for scenario in tqdm(data_idx[\"scenario\"].unique(),\n",
    "                             desc=\"Creating sequences\", leave=False):\n",
    "            idx = (data_idx[data_idx[\"scenario\"] == scenario]\n",
    "                   .sort_values(\"time\").index)\n",
    "            if len(idx) < sequence_length + 1:\n",
    "                continue\n",
    "            feat = feat_df.loc[idx].values\n",
    "            tgts = target_vectors[idx]\n",
    "            angs = self.original_angles[idx]\n",
    "            for i in range(len(idx) - sequence_length):\n",
    "                self.sequences.append(feat[i : i + sequence_length])\n",
    "                self.targets.append(tgts[i + sequence_length])\n",
    "                self.target_angles.append(angs[i + sequence_length])\n",
    "\n",
    "        self.sequences    = np.array(self.sequences,    dtype=np.float32)\n",
    "        self.targets      = np.array(self.targets,      dtype=np.float32)\n",
    "        self.target_angles= np.array(self.target_angles,dtype=np.float32)\n",
    "        print(f\"  Created {len(self.sequences):,} sequences\")\n",
    "\n",
    "    def __len__(self):  return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.sequences[idx]),\n",
    "                torch.FloatTensor(self.targets[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b70459",
   "metadata": {},
   "source": [
    "## 5 Â· Stratified Train / Val / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, train_ratio=0.70, val_ratio=0.15,\n",
    "                      test_ratio=0.15, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    train_sc, val_sc, test_sc = [], [], []\n",
    "    print(f\"{'Motion type':<20} {'total':>6} {'train':>6} {'val':>6} {'test':>6}\")\n",
    "    print(\"-\" * 48)\n",
    "    for mt in sorted(data[\"motion_type\"].unique()):\n",
    "        sc = data[data[\"motion_type\"] == mt][\"scenario\"].unique().copy()\n",
    "        rng.shuffle(sc)\n",
    "        n_tr = int(train_ratio * len(sc))\n",
    "        n_va = int(val_ratio   * len(sc))\n",
    "        train_sc.extend(sc[:n_tr])\n",
    "        val_sc.extend(sc[n_tr : n_tr + n_va])\n",
    "        test_sc.extend(sc[n_tr + n_va :])\n",
    "        print(f\"  {mt:<18} {len(sc):>6} {n_tr:>6} {n_va:>6} \"\n",
    "              f\"{len(sc)-n_tr-n_va:>6}\")\n",
    "    return train_sc, val_sc, test_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e07e3",
   "metadata": {},
   "source": [
    "## 6 Â· Model â€” Conv Front-End + RoPE Causal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e77c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ RoPE helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def _build_rope_cache(seq_len, head_dim, base=10000.0, device=\"cpu\", dtype=torch.float32):\n",
    "    theta = 1.0 / (base ** (torch.arange(0, head_dim, 2, device=device, dtype=dtype) / head_dim))\n",
    "    t = torch.arange(seq_len, device=device, dtype=dtype)\n",
    "    freqs = torch.einsum(\"t,f->tf\", t, theta)\n",
    "    return torch.cos(freqs), torch.sin(freqs)\n",
    "\n",
    "\n",
    "def _apply_rope(q, k, cos, sin):\n",
    "    q1, q2 = q[..., ::2], q[..., 1::2]\n",
    "    k1, k2 = k[..., ::2], k[..., 1::2]\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_rot = torch.stack([q1*cos - q2*sin, q1*sin + q2*cos], dim=-1).flatten(-2)\n",
    "    k_rot = torch.stack([k1*cos - k2*sin, k1*sin + k2*cos], dim=-1).flatten(-2)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "\n",
    "def _causal_mask(T, device):\n",
    "    return torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "\n",
    "# â”€â”€ Sub-modules â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class ConvFrontEnd(nn.Module):\n",
    "    def __init__(self, in_dim, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=5, stride=2, padding=2),\n",
    "            nn.GELU(), nn.BatchNorm1d(d_model),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GELU(), nn.BatchNorm1d(d_model),\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x).transpose(1, 2)\n",
    "        return self.drop(self.conv(h).transpose(1, 2))\n",
    "\n",
    "\n",
    "class RoPEAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, T, D = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        reshape = lambda z: z.view(B, T, self.n_heads, self.head_dim)\n",
    "        q, k, v = reshape(q), reshape(k), reshape(v)\n",
    "        cos, sin = _build_rope_cache(T, self.head_dim, device=x.device, dtype=x.dtype)\n",
    "        q, k = _apply_rope(q, k, cos, sin)\n",
    "        scores = torch.einsum(\"bthd,bThd->bhtT\", q, k) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "        attn = self.drop(torch.softmax(scores, dim=-1))\n",
    "        y = torch.einsum(\"bhtT,bThd->bthd\", attn, v).contiguous().view(B, T, D)\n",
    "        return self.out(y)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = RoPEAttention(d_model, n_heads, dropout)\n",
    "        self.ln1  = nn.LayerNorm(d_model)\n",
    "        self.ff   = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.ln2  = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = self.ln1(x + self.attn(x, attn_mask))\n",
    "        x = self.ln2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# â”€â”€ Top-level model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class CORTEXAnglePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    CORTEX Stage I.\n",
    "    Input : [B, T, F]\n",
    "    Output: [B, 6]  â€” two normalised 3-D unit vectors (TX, RX)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, n_layers=4,\n",
    "                 dim_feedforward=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.frontend = ConvFrontEnd(input_size, d_model, dropout)\n",
    "        self.blocks   = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dim_feedforward, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fuse = nn.Linear(d_model, d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model), nn.LayerNorm(d_model),\n",
    "            nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model // 2), nn.LayerNorm(d_model // 2),\n",
    "            nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 6),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.frontend(x)\n",
    "        causal = _causal_mask(h.size(1), h.device)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, causal)\n",
    "        last  = h[:, -1, :]\n",
    "        fused = last + 0.5 * self.fuse(h.mean(dim=1))\n",
    "        out   = self.head(fused)\n",
    "        tx, rx = out[:, :3], out[:, 3:]\n",
    "        tx = tx / (tx.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        rx = rx / (rx.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        return torch.cat([tx, rx], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a67ac",
   "metadata": {},
   "source": [
    "## 7 Â· Wrapped Cauchy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedCauchyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Circular-aware loss for paired 3-D unit vectors (TX + RX).\n",
    "    Uses the negative log-likelihood of a Wrapped Cauchy distribution\n",
    "    computed via cosine similarity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : float   Concentration âˆˆ (0,1). Higher = sharper.\n",
    "    \"\"\"\n",
    "    def __init__(self, rho=0.9, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.rho, self.eps = rho, eps\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_tx,  pred_rx  = pred[:, :3],   pred[:, 3:]\n",
    "        tgt_tx,   tgt_rx   = target[:, :3], target[:, 3:]\n",
    "        cos_tx = torch.sum(pred_tx * tgt_tx, dim=1).clamp(-1+self.eps, 1-self.eps)\n",
    "        cos_rx = torch.sum(pred_rx * tgt_rx, dim=1).clamp(-1+self.eps, 1-self.eps)\n",
    "        rho2   = self.rho ** 2\n",
    "        return torch.mean(\n",
    "            torch.log(1 + rho2 - 2*self.rho*cos_tx + self.eps) +\n",
    "            torch.log(1 + rho2 - 2*self.rho*cos_rx + self.eps)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41ca13",
   "metadata": {},
   "source": [
    "## 8 Â· Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEQUENCE_LENGTH = 15\n",
    "BATCH_SIZE      = 512\n",
    "\n",
    "# â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "D_MODEL    = 256\n",
    "N_HEADS    = 8\n",
    "N_LAYERS   = 4\n",
    "DIM_FF     = 512\n",
    "DROPOUT    = 0.2\n",
    "\n",
    "# â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS    = 100\n",
    "PATIENCE      = None    # Set an integer to enable early stopping, e.g. 15\n",
    "\n",
    "print(\"Hyperparameters set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86aaad",
   "metadata": {},
   "source": [
    "## 9 Â· Build Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc, val_sc, test_sc = stratified_split(data)\n",
    "\n",
    "train_data = data[data[\"scenario\"].isin(train_sc)].reset_index(drop=True)\n",
    "val_data   = data[data[\"scenario\"].isin(val_sc)].reset_index(drop=True)\n",
    "test_data  = data[data[\"scenario\"].isin(test_sc)].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBuilding datasets...\")\n",
    "train_ds = MobilityTimeSeriesDataset(train_data, SEQUENCE_LENGTH)\n",
    "val_ds   = MobilityTimeSeriesDataset(val_data,   SEQUENCE_LENGTH,\n",
    "               scaler=train_ds.scaler, label_encoder=train_ds.label_encoder)\n",
    "test_ds  = MobilityTimeSeriesDataset(test_data,  SEQUENCE_LENGTH,\n",
    "               scaler=train_ds.scaler, label_encoder=train_ds.label_encoder)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_ds):,}  |  Val: {len(val_ds):,}  |  Test: {len(test_ds):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672bc9a",
   "metadata": {},
   "source": [
    "## 10 Â· Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a29085",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_ds.sequences.shape[2]\n",
    "\n",
    "model = CORTEXAnglePredictor(\n",
    "    input_size=input_size,\n",
    "    d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,\n",
    "    dim_feedforward=DIM_FF, dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "total_p = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters : {total_p:,}  ({total_p*4/1024/1024:.1f} MB)\")\n",
    "print(f\"Input size : {input_size} features  |  Output: 6-D (2 Ã— 3-D unit vectors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771f832",
   "metadata": {},
   "source": [
    "## 11 Â· Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6506b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer, criterion, training=True):\n",
    "    model.train() if training else model.eval()\n",
    "    total = 0.0\n",
    "    ctx = torch.enable_grad() if training else torch.no_grad()\n",
    "    with ctx:\n",
    "        for seqs, tgts in tqdm(loader, desc=\"  train\" if training else \"  val  \",\n",
    "                               leave=False):\n",
    "            seqs, tgts = seqs.to(device), tgts.to(device)\n",
    "            preds = model(seqs)\n",
    "            loss  = criterion(preds, tgts)\n",
    "            if training:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            total += float(loss.detach())\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "\n",
    "criterion = WrappedCauchyLoss(rho=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val    = float(\"inf\")\n",
    "best_ckpt   = os.path.join(MODELS_DIR, \"stage1_best_model.pth\")\n",
    "no_improve  = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    tr = run_epoch(model, train_loader, optimizer, criterion, training=True)\n",
    "    va = run_epoch(model, val_loader,   optimizer, criterion, training=False)\n",
    "\n",
    "    old_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step(va)\n",
    "    new_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"  ğŸ“‰ LR {old_lr:.2e} â†’ {new_lr:.2e}\")\n",
    "\n",
    "    train_losses.append(tr)\n",
    "    val_losses.append(va)\n",
    "    print(f\"  Train: {tr:.6f}  |  Val: {va:.6f}\")\n",
    "\n",
    "    if va < best_val:\n",
    "        best_val  = va\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), best_ckpt)\n",
    "        print(f\"  âœ… Best saved (val={va:.6f})\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if PATIENCE and no_improve >= PATIENCE:\n",
    "            print(f\"\\nâš ï¸  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining complete. Best val loss: {best_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d971fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint before evaluation\n",
    "model.load_state_dict(torch.load(best_ckpt, map_location=device))\n",
    "print(f\"Best model loaded from: {best_ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dca51",
   "metadata": {},
   "source": [
    "## 12 Â· Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d09e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    for seqs, tgts in tqdm(loader, desc=\"  eval\", leave=False):\n",
    "        all_preds.append(model(seqs.to(device)).cpu().numpy())\n",
    "        all_tgts.append(tgts.numpy())\n",
    "\n",
    "    preds = np.vstack(all_preds)\n",
    "    tgts  = np.vstack(all_tgts)\n",
    "\n",
    "    pred_phi_tx,   pred_theta_tx  = unit_vectors_to_angles(preds[:, :3])\n",
    "    pred_phi_rx,   pred_theta_rx  = unit_vectors_to_angles(preds[:, 3:])\n",
    "    true_phi_tx,   true_theta_tx  = unit_vectors_to_angles(tgts[:, :3])\n",
    "    true_phi_rx,   true_theta_rx  = unit_vectors_to_angles(tgts[:, 3:])\n",
    "\n",
    "    pred_ang = np.stack([pred_phi_tx, pred_theta_tx, pred_phi_rx, pred_theta_rx], -1)\n",
    "    true_ang = np.stack([true_phi_tx, true_theta_tx, true_phi_rx, true_theta_rx], -1)\n",
    "\n",
    "    mae  = np.array([\n",
    "        circular_mae(pred_phi_tx,  true_phi_tx),\n",
    "        float(np.mean(np.abs(pred_theta_tx - true_theta_tx))),\n",
    "        circular_mae(pred_phi_rx,  true_phi_rx),\n",
    "        float(np.mean(np.abs(pred_theta_rx - true_theta_rx))),\n",
    "    ])\n",
    "    diff = np.abs(pred_ang - true_ang)\n",
    "    diff = np.where(diff > 180, 360 - diff, diff)\n",
    "    mse  = np.mean(diff**2, axis=0)\n",
    "    rmse = np.sqrt(mse)\n",
    "    ss_r = np.sum((true_ang - pred_ang)**2, axis=0)\n",
    "    ss_t = np.sum((true_ang - true_ang.mean(0))**2, axis=0)\n",
    "    r2   = 1 - ss_r / (ss_t + 1e-8)\n",
    "\n",
    "    ang_tx = np.rad2deg(np.arccos(np.clip(\n",
    "        np.sum(preds[:, :3] * tgts[:, :3], axis=1), -1, 1)))\n",
    "    ang_rx = np.rad2deg(np.arccos(np.clip(\n",
    "        np.sum(preds[:, 3:] * tgts[:, 3:], axis=1), -1, 1)))\n",
    "\n",
    "    names = [\"phi_tx\", \"theta_tx\", \"phi_rx\", \"theta_rx\"]\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  {'Angle':<16} {'MSE':>10} {'RMSE':>10} {'MAE (Â°)':>10} {'RÂ²':>10}\")\n",
    "    print(\"  \" + \"-\"*50)\n",
    "    for i, n in enumerate(names):\n",
    "        print(f\"  {n:<16} {mse[i]:10.4f} {rmse[i]:10.4f} {mae[i]:10.4f} {r2[i]:10.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  Angular dist TX : {ang_tx.mean():.4f}Â° Â± {ang_tx.std():.4f}Â°\")\n",
    "    print(f\"  Angular dist RX : {ang_rx.mean():.4f}Â° Â± {ang_rx.std():.4f}Â°\")\n",
    "    print(f\"  Overall MAE     : {mae.mean():.4f}Â°\")\n",
    "    print(f\"  Overall RÂ²      : {r2.mean():.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return pred_ang, true_ang, mse, mae, r2\n",
    "\n",
    "predictions, targets_eval, mse, mae, r2 = evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e0332",
   "metadata": {},
   "source": [
    "## 13 Â· Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANGLE_NAMES = [\"phi_tx_optimal\", \"theta_tx_optimal\",\n",
    "               \"phi_rx_optimal\", \"theta_rx_optimal\"]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Loss curve\n",
    "ax = fig.add_subplot(3, 3, 1)\n",
    "ax.plot(train_losses, label=\"Train\", lw=2)\n",
    "ax.plot(val_losses,   label=\"Val\",   lw=2)\n",
    "ax.set(xlabel=\"Epoch\", ylabel=\"Loss\",\n",
    "       title=\"Training & Validation Loss\")\n",
    "ax.set_yscale(\"log\"); ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "# Scatter: predicted vs true\n",
    "idx = np.random.choice(len(predictions), min(1000, len(predictions)), replace=False)\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(3, 3, i + 2)\n",
    "    ax.scatter(targets_eval[idx, i], predictions[idx, i],\n",
    "               alpha=0.5, s=15, edgecolors=\"none\")\n",
    "    lo = min(targets_eval[:, i].min(), predictions[:, i].min())\n",
    "    hi = max(targets_eval[:, i].max(), predictions[:, i].max())\n",
    "    ax.plot([lo, hi], [lo, hi], \"r--\", lw=2, label=\"Perfect\")\n",
    "    ax.set(xlabel=\"True (Â°)\", ylabel=\"Predicted (Â°)\",\n",
    "           title=ANGLE_NAMES[i])\n",
    "    ax.legend(fontsize=9); ax.grid(alpha=0.3)\n",
    "\n",
    "# Error histograms\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(3, 3, i + 6)\n",
    "    errs = predictions[:, i] - targets_eval[:, i]\n",
    "    if \"phi\" in ANGLE_NAMES[i]:\n",
    "        errs = (errs + 180) % 360 - 180\n",
    "    ax.hist(errs, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "    ax.axvline(0, color=\"red\", linestyle=\"--\", lw=2)\n",
    "    ax.set(xlabel=\"Error (Â°)\", ylabel=\"Frequency\",\n",
    "           title=f\"{ANGLE_NAMES[i]} â€” Error Dist.\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(PLOTS_DIR, \"stage1_training_results.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Plot saved â†’ {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68da02b",
   "metadata": {},
   "source": [
    "## 14 Â· Save Final Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Full checkpoint (model + preprocessing objects + metadata)\n",
    "final_ckpt = os.path.join(MODELS_DIR, f\"stage1_model_{ts}.pth\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"scaler\":           train_ds.scaler,\n",
    "    \"label_encoder\":    train_ds.label_encoder,\n",
    "    \"hyperparameters\": {\n",
    "        \"input_size\": input_size,\n",
    "        \"sequence_length\": SEQUENCE_LENGTH,\n",
    "        \"d_model\": D_MODEL, \"n_heads\": N_HEADS,\n",
    "        \"n_layers\": N_LAYERS, \"dim_feedforward\": DIM_FF,\n",
    "        \"dropout\": DROPOUT,\n",
    "    },\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\":   val_losses,\n",
    "    \"test_metrics\": {\n",
    "        \"mae\":  mae.tolist(),\n",
    "        \"mse\":  mse.tolist(),\n",
    "        \"r2\":   r2.tolist(),\n",
    "    },\n",
    "    \"timestamp\": ts,\n",
    "}, final_ckpt)\n",
    "\n",
    "# Human-readable metrics JSON\n",
    "metrics_path = os.path.join(RESULTS_DIR, f\"stage1_metrics_{ts}.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"overall_mae\":  float(mae.mean()),\n",
    "        \"overall_r2\":   float(r2.mean()),\n",
    "        \"best_val_loss\": float(min(val_losses)),\n",
    "        \"mae_per_angle\": dict(zip(\n",
    "            [\"phi_tx\", \"theta_tx\", \"phi_rx\", \"theta_rx\"],\n",
    "            [float(x) for x in mae])),\n",
    "        \"r2_per_angle\":  dict(zip(\n",
    "            [\"phi_tx\", \"theta_tx\", \"phi_rx\", \"theta_rx\"],\n",
    "            [float(x) for x in r2])),\n",
    "        \"timestamp\": ts,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Model   â†’ {final_ckpt}\")\n",
    "print(f\"Metrics â†’ {metrics_path}\")\n",
    "print(f\"\\nBest val loss : {min(val_losses):.6f}\")\n",
    "print(f\"Test MAE      : {mae.mean():.4f}Â°\")\n",
    "print(f\"Test RÂ²       : {r2.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1e14f",
   "metadata": {},
   "source": [
    "## 15 Â· Loading a Checkpoint (for Stage II)\n",
    "\n",
    "Use this snippet to reload the model in another notebook or script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reload the saved model\n",
    "ckpt = torch.load(final_ckpt, map_location=device)\n",
    "hp   = ckpt[\"hyperparameters\"]\n",
    "\n",
    "loaded_model = CORTEXAnglePredictor(\n",
    "    input_size     = hp[\"input_size\"],\n",
    "    d_model        = hp[\"d_model\"],\n",
    "    n_heads        = hp[\"n_heads\"],\n",
    "    n_layers       = hp[\"n_layers\"],\n",
    "    dim_feedforward= hp[\"dim_feedforward\"],\n",
    "    dropout        = hp[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "loaded_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "loaded_model.eval()\n",
    "\n",
    "scaler        = ckpt[\"scaler\"]          # StandardScaler â€” pass to val/test datasets\n",
    "label_encoder = ckpt[\"label_encoder\"]  # LabelEncoder   â€” pass to val/test datasets\n",
    "\n",
    "print(\"Model reloaded successfully.\")\n",
    "print(f\"Sequence length: {hp['sequence_length']}\")\n",
    "print(f\"Input size:      {hp['input_size']}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
